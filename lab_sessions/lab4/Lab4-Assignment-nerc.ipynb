{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab4-Assignment about Named Entity Recognition and Classification\n",
    "\n",
    "This notebook describes the assignment of Lab 4 of the text mining course. We assume you have succesfully completed Lab1, Lab2 and Lab3 as welll. Especially Lab2 is important for completing this assignment.\n",
    "\n",
    "**Learning goals**\n",
    "* going from linguistic input format to representing it in a feature space\n",
    "* working with pretrained word embeddings\n",
    "* train a supervised classifier (SVM)\n",
    "* evaluate a supervised classifier (SVM)\n",
    "* learn how to interpret the system output and the evaluation results\n",
    "* be able to propose future improvements based on the observed results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "This notebook was originally created by [Marten Postma](https://martenpostma.github.io) and [Filip Ilievski](http://ilievski.nl) and adapted by Piek vossen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 18] Exercise 1 (NERC): Training and evaluating an SVM using CoNLL-2003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 point] a) Load the CoNLL-2003 training data using the *ConllCorpusReader* and create for both *train.txt* and *test.txt*:**\n",
    "\n",
    "    [2 points]  -a list of dictionaries representing the features for each training instances, e..g,\n",
    "    ```\n",
    "    [\n",
    "    {'words': 'EU', 'pos': 'NNP'}, \n",
    "    {'words': 'rejects', 'pos': 'VBZ'},\n",
    "    ...\n",
    "    ]\n",
    "    ```\n",
    "\n",
    "    [2 points] -the NERC labels associated with each training instance, e.g.,\n",
    "    dictionaries, e.g.,\n",
    "    ```\n",
    "    [\n",
    "    'B-ORG', \n",
    "    'O',\n",
    "    ....\n",
    "    ]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "train = ConllCorpusReader('./CONLL2003/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for token, pos, ne_label in train.iob_words():\n",
    "   a_dict = {\n",
    "      \"word.lower()\": token.lower(),\n",
    "      \"word.isupper()\": token.isupper(),\n",
    "      \"word.istitle()\": token.istitle(),\n",
    "      \"word.isdigit()\": token.isdigit(),\n",
    "      \"postag\": pos,\n",
    "      \"word.shape\": ''.join(['X' if c.isupper() else 'X' if c.islower() else 'd' if c.isdigit() else '-' for c in token])\n",
    "   }\n",
    "   training_features.append(a_dict)\n",
    "   training_gold_labels.append(ne_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "test = ConllCorpusReader('./CONLL2003/CONLL2003', 'test.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "test_features = []\n",
    "test_gold_labels = []\n",
    "for token, pos, ne_label in test.iob_words():\n",
    "    a_dict = {\n",
    "        \"word.lower()\": token.lower(),\n",
    "        \"word.isupper()\": token.isupper(),\n",
    "        \"word.istitle()\": token.istitle(),\n",
    "        \"word.isdigit()\": token.isdigit(),\n",
    "        \"postag\": pos,\n",
    "        \"word.shape\": ''.join(['X' if c.isupper() else 'X' if c.islower() else 'd' if c.isdigit() else '-' for c in token])\n",
    "    }\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(ne_label)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] b) provide descriptive statistics about the training and test data:**\n",
    "* How many instances are in train and test?\n",
    "* Provide a frequency distribution of the NERC labels, i.e., how many times does each NERC label occur?\n",
    "* Discuss to what extent the training and test data is balanced (equal amount of instances for each NERC label) and to what extent the training and test data differ?\n",
    "\n",
    "Tip: you can use the following `Counter` functionality to generate frequency list of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 2, 2: 2, 3: 1, 5: 1})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter \n",
    "\n",
    "my_list=[1,2,1,3,2,5]\n",
    "Counter(my_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[2 points] c) Concatenate the train and test features (the list of dictionaries) into one list. Load it using the *DictVectorizer*. Afterwards, split it back to training and test.**\n",
    "\n",
    "Tip: You’ve concatenated train and test into one list and then you’ve applied the DictVectorizer.\n",
    "The order of the rows is maintained. You can hence use an index (number of training instances) to split the_array back into train and test. Do NOT use: `\n",
    "from sklearn.model_selection import train_test_split` here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade numpy==1.26.4\n",
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = DictVectorizer()\n",
    "\n",
    "# Concatenate train and test features\n",
    "combined_features = training_features + test_features\n",
    "\n",
    "# Initialize the DictVectorizer\n",
    "vec = DictVectorizer()\n",
    "\n",
    "# Fit the vectorizer on the combined features\n",
    "the_array = vec.fit_transform(combined_features)\n",
    "\n",
    "# Store the number of training instances to use as the split index\n",
    "num_train_instances = len(training_features)\n",
    "\n",
    "# Split the array back into training and test sets using the index\n",
    "X_train_vectorized = the_array[:num_train_instances]\n",
    "X_test_vectorized = the_array[num_train_instances:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] d) Train the SVM using the train features and labels and evaluate on the test data. Provide a classification report (sklearn.metrics.classification_report).**\n",
    "The train (*lin_clf.fit*) might take a while. On my computer, it took 1min 53s, which is acceptable. Training models normally takes much longer. If it takes more than 5 minutes, you can use a subset for training. Describe the results:\n",
    "* Which NERC labels does the classifier perform well on? Why do you think this is the case?\n",
    "\n",
    "The classifier performs well on the 'O' label because it is the most frequent label.\n",
    "* Which NERC labels does the classifier perform poorly on? Why do you think this is the case?\n",
    "The classifier might perform poorly on less frequent labels like 'I-LOC', 'I-MISC' due to insufficient training examples.\n",
    "\n",
    "<i>If you run with the complete CONLL data, you will get a more detailed report that allows proper analysis of the model's strengths and weaknesses.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_clf = svm.LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.78      0.79      0.79      1668\n",
      "      B-MISC       0.72      0.69      0.70       702\n",
      "       B-ORG       0.71      0.57      0.64      1661\n",
      "       B-PER       0.64      0.58      0.61      1617\n",
      "       I-LOC       0.64      0.56      0.60       257\n",
      "      I-MISC       0.63      0.58      0.61       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.43      0.62      0.50      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.69      0.65      0.66     46435\n",
      "weighted avg       0.93      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the classifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "lin_clf.fit(X_train_vectorized, training_gold_labels)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lin_clf.predict(X_test_vectorized)\n",
    "\n",
    "# Generate the classification report\n",
    "report = classification_report(test_gold_labels, y_pred)\n",
    "\n",
    "# Print the classification report\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[6 points] e) Train a model that uses the embeddings of these words as inputs. Test again on the same data as in 2d. Generate a classification report and compare the results with the classifier you built in 2d.**\n",
    "\n",
    "# Differences between the two reports:\n",
    "\n",
    "For the B-Loc, in the second results we can see that the precision increases with 0,01. There is a major significant difference, since all the values are the same expect precision. The second report highlighted a increase of 0,01 in precision over the first, while recall and f1-score stay consistant. For B-ORG and B-PER there are no differences in the both reports. I-LOC has a minor drop in precision, I-MISC has a slightly decrease in F1-score and I-Org has a decrease in recall of also 0,01. On the other hand, I-PER has a slightly increase in the F1-score of 0,01 and finally O did not had any changes in both results.\n",
    "\n",
    "Overall, the two classification reports both have small differences in the overall preformance, since the macro avg, weighted avg and accurcy stayed the same over the two measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.78      0.79      0.79      1668\n",
      "      B-MISC       0.72      0.69      0.70       702\n",
      "       B-ORG       0.71      0.57      0.64      1661\n",
      "       B-PER       0.64      0.58      0.61      1617\n",
      "       I-LOC       0.64      0.56      0.60       257\n",
      "      I-MISC       0.63      0.58      0.61       216\n",
      "       I-ORG       0.66      0.48      0.55       835\n",
      "       I-PER       0.43      0.62      0.50      1156\n",
      "           O       0.98      0.99      0.99     38323\n",
      "\n",
      "    accuracy                           0.93     46435\n",
      "   macro avg       0.69      0.65      0.66     46435\n",
      "weighted avg       0.93      0.93      0.92     46435\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "clf = LinearSVC(max_iter=1000)\n",
    "clf.fit(X_train_vectorized, training_gold_labels)\n",
    "\n",
    "\n",
    "predicted_labels = clf.predict(X_test_vectorized)\n",
    "\n",
    "\n",
    "print(classification_report(test_gold_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Points: 10] Exercise 2 (NERC): feature inspection using the [Annotated Corpus for Named Entity Recognition](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus)\n",
    "**[6 points] a. Perform the same steps as in the previous exercise. Make sure you end up for both the training part (*df_train*) and the test part (*df_test*) with:**\n",
    "* the features representation using **DictVectorizer**\n",
    "* the NERC labels in a list\n",
    "\n",
    "Please note that this is the same setup as in the previous exercise:\n",
    "* load both train and test using:\n",
    "    * list of dictionaries for features\n",
    "    * list of NERC labels\n",
    "* combine train and test features in a list and represent them using one hot encoding\n",
    "* train using the training features and NERC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Adapt the path to point to your local copy of NERC_datasets\n",
    "path = 'ner_dataset.csv'\n",
    "kaggle_dataset = pandas.read_csv(path, on_bad_lines='skip', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1048575"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kaggle_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 20000\n"
     ]
    }
   ],
   "source": [
    "df_train = kaggle_dataset[:100000]\n",
    "df_test = kaggle_dataset[100000:120000]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[4 points] b. Train and evaluate the model and provide the classification report:**\n",
    "* use the SVM to predict NERC labels on the test data\n",
    "* evaluate the performance of the SVM on the test data\n",
    "\n",
    "Analyze the performance per NERC label.\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Subset: you can change the sizes as needed\n",
    "df_train = df[:100000]\n",
    "df_test = df[100000:115000]\n",
    "\n",
    "# Extract features and labels\n",
    "X_train = df_train['Word'].fillna('').values\n",
    "X_test = df_test['Word'].fillna('').values\n",
    "\n",
    "y_train = df_train['Tag'].fillna('O').values\n",
    "y_test = df_test['Tag'].fillna('O').values\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_enc = label_encoder.fit_transform(y_train)\n",
    "y_test_enc = label_encoder.transform(y_test)\n",
    "\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train_vec, y_train_enc)\n",
    "\n",
    "y_pred_enc = clf.predict(X_test_vec)\n",
    "\n",
    "y_pred = label_encoder.inverse_transform(y_pred_enc)\n",
    "y_test_labels = label_encoder.inverse_transform(y_test_enc)\n",
    "\n",
    "report = classification_report(y_test_labels, y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "```\n",
    "\n",
    "An overall 92% accuracy was achieved by the SVM model. However, the performance varied a lot across different NERC labels. Where it performed extremely well was in the ‘O’, “non-entity” class. The precision was 0.94 and the recall was 0.99. This was likely because of the large number of non-entity tokens in the dataset. Strong performances were also shown for the geo-political entities, in short B-gpe and time expressions, B-tim. The f1-scores were 0.91 and 0.79. The SVM did struggle with entities that were less frequent like the B-art, B-eve, I-art and B-nat. The f1-scores for those were either 0.00 or very low. This was because of limited examples and poor recognition. Another thing that showed a lower recall were Inside tags, I-labels. Especially I-geo and I-tim. This suggests that there was difficulty with the recognition of multi-token or nested entities. These imbalances are highlighted by the macro average F1-score of 0.45. On the contrary, the weighted average F1-score, 0.91, is lopsided by the strong performance in the ‘O’ class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PCA25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
