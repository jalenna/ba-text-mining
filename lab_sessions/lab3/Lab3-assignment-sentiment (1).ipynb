{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Lab3 - Assignment Sentiment"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "This notebook describes the LAB-3 assignment of the Text Mining course. It is about sentiment analysis.\n",
                "\n",
                "The aims of the assignment are:\n",
                "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
                "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
                "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
                "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
                "* Learn how to evaluate the results qualitatively (by examining the data) \n",
                "* Get insight into differences between the two applied methods\n",
                "* Get insight into the effects of using linguistic preprocessing\n",
                "* Be able to describe differences between the two methods in terms of their results\n",
                "* Get insight into issues when applying these methods across different  domains\n",
                "\n",
                "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
                "\n",
                "We recommend you go through the notebooks in the following order:\n",
                "* **Read the assignment (see below)**\n",
                "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
                "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
                "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
                "\n",
                "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
                "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
                "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Credits\n",
                "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part I: VADER assignments\n",
                "\n",
                "\n",
                "### Preparation (nothing to submit):\n",
                "To be able to answer the VADER questions you need to know how the tool works. \n",
                "* Read more about the VADER tool in [this blog](https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/).  \n",
                "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
                "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
                "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
                "\n",
                "\n",
                "### [3.5 points] Question1:\n",
                "\n",
                "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
                "\n",
                "```\n",
                "INPUT SENTENCE 1 I love apples\n",
                "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
                "\n",
                "INPUT SENTENCE 2 I don't love apples\n",
                "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
                "\n",
                "INPUT SENTENCE 3 I love apples :-)\n",
                "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
                "\n",
                "INPUT SENTENCE 4 These houses are ruins\n",
                "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
                "\n",
                "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
                "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
                "\n",
                "INPUT SENTENCE 6 He lies in the chair in the garden\n",
                "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
                "\n",
                "INPUT SENTENCE 7 This house is like any house\n",
                "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
                "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream. If you have trouble accessing Twitter, try to find an existing dataset (on websites like kaggle or huggingface).\n",
                "\n",
                "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
                "For each tweet, you should insert:\n",
                "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
                "* the text of the tweet\n",
                "* the Tweet-URL\n",
                "\n",
                "from:\n",
                "```\n",
                "    \"1\": {\n",
                "        \"sentiment_label\": \"\",\n",
                "        \"text_of_tweet\": \"\",\n",
                "        \"tweet_url\": \"\",\n",
                "```\n",
                "to:\n",
                "```\n",
                "\"1\": {\n",
                "        \"sentiment_label\": \"positive\",\n",
                "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
                "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
                "    },\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can load your tweets with human annotation in the following way."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "my_tweets = None\n",
                "with open('my_tweets.json', encoding=\"utf8\") as f:\n",
                "    my_tweets = json.load(f)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1 {'sentiment_label': 'Positive', 'text_of_tweet': 'Be sure to tune in and watch Donald Trump on Late Night with David Letterman as he presents the Top Ten List tonight!', 'tweet_url': 'https://twitter.com/realDonaldTrump/status/1698308935'}\n"
                    ]
                }
            ],
            "source": [
                "for id_, tweet_info in my_tweets.items():\n",
                "    print(id_, tweet_info)\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [5 points] Question 3:\n",
                "\n",
                "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
                "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
                "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "def vader_output_to_label(vader_output):\n",
                "    \"\"\"\n",
                "    map vader output e.g.,\n",
                "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
                "    to one of the following values:\n",
                "    a) positive float -> 'positive'\n",
                "    b) 0.0 -> 'neutral'\n",
                "    c) negative float -> 'negative'\n",
                "    \n",
                "    :param dict vader_output: output dict from vader\n",
                "    \n",
                "    :rtype: str\n",
                "    :return: 'negative' | 'neutral' | 'positive'\n",
                "    \"\"\"\n",
                "    compound = vader_output['compound']\n",
                "    \n",
                "    if compound < 0:\n",
                "        return 'negative'\n",
                "    elif compound == 0.0:\n",
                "        return 'neutral'\n",
                "    elif compound > 0.0:\n",
                "        return 'positive'\n",
                "    \n",
                "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
                "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
                "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.50      0.33      0.40        12\n",
                        "     neutral       0.60      0.45      0.51        20\n",
                        "    positive       0.48      0.72      0.58        18\n",
                        "\n",
                        "    accuracy                           0.52        50\n",
                        "   macro avg       0.53      0.50      0.50        50\n",
                        "weighted avg       0.53      0.52      0.51        50\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "analyzer = SentimentIntensityAnalyzer()\n",
                "tweets = []\n",
                "all_vader_output = []\n",
                "gold = []\n",
                "\n",
                "# settings (to change for different experiments)\n",
                "to_lemmatize = True \n",
                "pos = set()\n",
                "\n",
                "for id_, tweet_info in my_tweets.items():\n",
                "    the_tweet = tweet_info['text_of_tweet']\n",
                "    vader_output = analyzer.polarity_scores(the_tweet) # run vader\n",
                "    vader_label = vader_output_to_label(vader_output) # convert vader output to category\n",
                "    \n",
                "    tweets.append(the_tweet)\n",
                "    all_vader_output.append(vader_label)\n",
                "    gold.append(tweet_info['sentiment_label'].strip().lower()) # Since the json file contained capitilized labels.\n",
                "    \n",
                "# use scikit-learn's classification report\n",
                "print(classification_report(gold, all_vader_output))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3A:\n",
                " \n",
                "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
                "\n",
                "The precision measures for each lable (negative, neutral, positive), how many predictions actually were accruate on target. \n",
                "\n",
                "For negative, 50% of the tweet predictions that were marked as negative were actually negative.\n",
                "For neutral, 60% of the tweet predictions that were marked as neutral were actually neutral.\n",
                "For positive, 48% of the tweet predictions that were marked as positive were actually postive.\n",
                "\n",
                "The recall measures for each lable (negative, neutral, positive) the ability of idenitfing the correct label based on the tweet information.\n",
                "\n",
                "For negative, 33% of the negative tweets were correcly indentified.\n",
                "For neutral, 45% of the neutral tweets were correcly indentified.\n",
                "For positive, 72% of the postive tweets were correcly indentified.\n",
                "\n",
                "The f1-score measures for each lable (negative, neutral, positive) the mean of precision and recall. f1-score indicates a balance measurement.\n",
                "\n",
                "For negative, 40% highlights a poorly preformance.\n",
                "For neutral, 51% highlights a monderate preformance.\n",
                "For positive, 58% highlights a better preformance relatively.\n",
                "\n",
                "\n",
                "The support provides an overview of the acrtual amount of negativve, neautral and positve(12,20,18) tweets.\n",
                "\n",
                "Regarding the accuracy, the model was able to correctly classify 52% of all the tweets.\n",
                "\n",
                "Macro avg represents the average metrics equally over the classes, while weighted avg illustrats the average meterics weighted by support.\n",
                "\n",
                "# Most relevant scores:\n",
                "F1-score provided a view over the balace between precision and recall, this is important when classes are imbalanaced.\n",
                "Weighted avg provided a more accruacte view, since wieghted avg took imbalancing inaccount compared to macro avg.\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 3B:\n",
                " \n",
                "\n",
                "\n",
                "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Collecting pandas\n",
                        "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
                        "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\romai\\desktop\\ba-text-mining-master\\.conda\\lib\\site-packages (from pandas) (2.2.5)\n",
                        "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\romai\\desktop\\ba-text-mining-master\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
                        "Collecting pytz>=2020.1 (from pandas)\n",
                        "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
                        "Collecting tzdata>=2022.7 (from pandas)\n",
                        "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
                        "Requirement already satisfied: six>=1.5 in c:\\users\\romai\\desktop\\ba-text-mining-master\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
                        "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
                        "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
                        "   ------------------------------- -------- 9.2/11.5 MB 51.8 MB/s eta 0:00:01\n",
                        "   ---------------------------------------  11.3/11.5 MB 35.3 MB/s eta 0:00:01\n",
                        "   ---------------------------------------  11.3/11.5 MB 35.3 MB/s eta 0:00:01\n",
                        "   ---------------------------------------  11.3/11.5 MB 35.3 MB/s eta 0:00:01\n",
                        "   ---------------------------------------  11.3/11.5 MB 35.3 MB/s eta 0:00:01\n",
                        "   ---------------------------------------- 11.5/11.5 MB 9.9 MB/s eta 0:00:00\n",
                        "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
                        "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
                        "Installing collected packages: pytz, tzdata, pandas\n",
                        "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
                    ]
                }
            ],
            "source": [
                "!pip install pandas\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "\n",
                "df = pd.DataFrame({\n",
                "    'text': tweets,\n",
                "    'true_label': gold,\n",
                "    'vader_prediction': all_vader_output\n",
                "})\n",
                "\n",
                "mistakes = df[df['true_label'] != df['vader_prediction']]\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": [
                "error_samples = {}\n",
                "\n",
                "for sentiment in ['positive', 'negative', 'neutral']:\n",
                "    subset = mistakes[mistakes['true_label'] == sentiment]\n",
                "    error_samples[sentiment] = subset.head(10)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- POSITIVE Errors ---\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>true_label</th>\n",
                            "      <th>vader_prediction</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>9</th>\n",
                            "      <td>\"When the achiever achieves, it's not a platea...</td>\n",
                            "      <td>positive</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>13</th>\n",
                            "      <td>\"Always know you could be on the precipice of ...</td>\n",
                            "      <td>positive</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>28</th>\n",
                            "      <td>“Expand your life every day.” –Donald J. Trump...</td>\n",
                            "      <td>positive</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>43</th>\n",
                            "      <td>- Donald Trump bids to buy the Oreo Double Stu...</td>\n",
                            "      <td>positive</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>44</th>\n",
                            "      <td>- More hysterical DSRL videos featuring Donald...</td>\n",
                            "      <td>positive</td>\n",
                            "      <td>negative</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 text true_label  \\\n",
                            "9   \"When the achiever achieves, it's not a platea...   positive   \n",
                            "13  \"Always know you could be on the precipice of ...   positive   \n",
                            "28  “Expand your life every day.” –Donald J. Trump...   positive   \n",
                            "43  - Donald Trump bids to buy the Oreo Double Stu...   positive   \n",
                            "44  - More hysterical DSRL videos featuring Donald...   positive   \n",
                            "\n",
                            "   vader_prediction  \n",
                            "9           neutral  \n",
                            "13          neutral  \n",
                            "28          neutral  \n",
                            "43          neutral  \n",
                            "44         negative  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- NEGATIVE Errors ---\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>true_label</th>\n",
                            "      <th>vader_prediction</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>3</th>\n",
                            "      <td>@ charleyUfarley You mean like Buffett, Kravis...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>4</th>\n",
                            "      <td>\"My persona will never be that of a wallflower...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>17</th>\n",
                            "      <td>\" @ DominicFormaro: @ realDonaldTrump it's hil...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>22</th>\n",
                            "      <td>“If you don't have problems, you're pretending...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>33</th>\n",
                            "      <td>\"Be aware of things that seem inexplicable bec...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>neutral</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>39</th>\n",
                            "      <td>China does not negotiate from a position of st...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>47</th>\n",
                            "      <td>China is neither an ally or a friend--they wan...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>49</th>\n",
                            "      <td>If China had a tenth of the natural resources ...</td>\n",
                            "      <td>negative</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 text true_label  \\\n",
                            "3   @ charleyUfarley You mean like Buffett, Kravis...   negative   \n",
                            "4   \"My persona will never be that of a wallflower...   negative   \n",
                            "17  \" @ DominicFormaro: @ realDonaldTrump it's hil...   negative   \n",
                            "22  “If you don't have problems, you're pretending...   negative   \n",
                            "33  \"Be aware of things that seem inexplicable bec...   negative   \n",
                            "39  China does not negotiate from a position of st...   negative   \n",
                            "47  China is neither an ally or a friend--they wan...   negative   \n",
                            "49  If China had a tenth of the natural resources ...   negative   \n",
                            "\n",
                            "   vader_prediction  \n",
                            "3          positive  \n",
                            "4           neutral  \n",
                            "17         positive  \n",
                            "22         positive  \n",
                            "33          neutral  \n",
                            "39         positive  \n",
                            "47         positive  \n",
                            "49         positive  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "--- NEUTRAL Errors ---\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "<div>\n",
                            "<style scoped>\n",
                            "    .dataframe tbody tr th:only-of-type {\n",
                            "        vertical-align: middle;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe tbody tr th {\n",
                            "        vertical-align: top;\n",
                            "    }\n",
                            "\n",
                            "    .dataframe thead th {\n",
                            "        text-align: right;\n",
                            "    }\n",
                            "</style>\n",
                            "<table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            "    <tr style=\"text-align: right;\">\n",
                            "      <th></th>\n",
                            "      <th>text</th>\n",
                            "      <th>true_label</th>\n",
                            "      <th>vader_prediction</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <th>6</th>\n",
                            "      <td>Listen to an interview with Donald Trump discu...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>8</th>\n",
                            "      <td>Enter the \"Think Like A Champion\" signed book ...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>16</th>\n",
                            "      <td>Don't forget to enter the \"Think Like A Champi...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>18</th>\n",
                            "      <td>\"Your higher self is in direct opposition to y...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>19</th>\n",
                            "      <td>Read an excerpt from Think Like A Champion by ...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>23</th>\n",
                            "      <td>Last week to enter the \"Think Like A Champion\"...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>26</th>\n",
                            "      <td>RE: FB Vanity URLs: SF Chronicle - \"David Beck...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>negative</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>37</th>\n",
                            "      <td>Browse Donald Trump's Summer Reading List for ...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>positive</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>41</th>\n",
                            "      <td>Watch the Miss Universe competition LIVE from ...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>negative</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <th>42</th>\n",
                            "      <td>Reminder: The Miss Universe competition will b...</td>\n",
                            "      <td>neutral</td>\n",
                            "      <td>negative</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table>\n",
                            "</div>"
                        ],
                        "text/plain": [
                            "                                                 text true_label  \\\n",
                            "6   Listen to an interview with Donald Trump discu...    neutral   \n",
                            "8   Enter the \"Think Like A Champion\" signed book ...    neutral   \n",
                            "16  Don't forget to enter the \"Think Like A Champi...    neutral   \n",
                            "18  \"Your higher self is in direct opposition to y...    neutral   \n",
                            "19  Read an excerpt from Think Like A Champion by ...    neutral   \n",
                            "23  Last week to enter the \"Think Like A Champion\"...    neutral   \n",
                            "26  RE: FB Vanity URLs: SF Chronicle - \"David Beck...    neutral   \n",
                            "37  Browse Donald Trump's Summer Reading List for ...    neutral   \n",
                            "41  Watch the Miss Universe competition LIVE from ...    neutral   \n",
                            "42  Reminder: The Miss Universe competition will b...    neutral   \n",
                            "\n",
                            "   vader_prediction  \n",
                            "6          positive  \n",
                            "8          positive  \n",
                            "16         positive  \n",
                            "18         positive  \n",
                            "19         positive  \n",
                            "23         positive  \n",
                            "26         negative  \n",
                            "37         positive  \n",
                            "41         negative  \n",
                            "42         negative  "
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "for sentiment, errors in error_samples.items():\n",
                "    print(f\"\\n--- {sentiment.upper()} Errors ---\")\n",
                "    display(errors[['text', 'true_label', 'vader_prediction']])\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "positive tweet predicted as neutral:\n",
                "\"Expand your life every day.\"\n",
                "why wrong? its motivational but vader doesn't really pick that up because there aren't strong positive words in it. maybe too short too.\n",
                "\n",
                "negative tweet predicited as positive:\n",
                "\"China does not negotiate from a position of strength, we simply negotiate against ourselves.\"\n",
                "why wrong? it's obviously negative, he's criticising something, but vader doesn't see it cause the words are kinda formal and not really 'angry' in vader's dictionary.\n",
                "\n",
                "neutral tweet predicted as positive:\n",
                "\"Don't forget to enter the 'Think Like A Champion' signed book and keychain contest.\"\n",
                "why wrong? vader probably sees \"champion\" and \"contest\" as exciting/positive, but this tweet is just like an ad. it's neutral in my opinion. \n",
                "\n",
                "most of the time vader just looks at the words without knowing what the whole sentence means. so it get it whole wrong. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [4 points] Question 4:\n",
                "Run VADER on the set of airline tweets with the following settings:\n",
                "\n",
                "* Run VADER (as it is) on the set of airline tweets \n",
                "* Run VADER on the set of airline tweets after having lemmatized the text\n",
                "* Run VADER on the set of airline tweets with only adjectives\n",
                "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
                "* Run VADER on the set of airline tweets with only nouns\n",
                "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
                "* Run VADER on the set of airline tweets with only verbs\n",
                "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
                "\n",
                "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
                "* [3 points] b. Compare the scores and explain what they tell you.\n",
                "* - Does lemmatisation help? Explain why or why not.\n",
                "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not.\n",
                "\n",
                "Lemmatisation doesn't help in these cases, and for sentiment analysis in general because the tenses matter. \"Hate\" is quite different from \"hated,\" but you lose that meaning through reducing it to its lemma.\n",
                "Based on the results of the tests it appears that no not all parts of speech are equally important, this may be due to the weights certain words(part of speech) carry. While it is true that certain keywords carry sentimental meaning, in combination with other parts of speech that meaning may change, thus affecting the overal sentiment."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "# nltk.download()\n",
                "import nltk\n",
                "import pathlib\n",
                "from sklearn.datasets import load_files\n",
                "\n",
                "cwd = pathlib.Path.cwd()\n",
                "airline_tweets_folder = cwd.joinpath('airlinetweets/airlinetweets')\n",
                "# loading all files as training data.\n",
                "airline_tweets_train = load_files(str(airline_tweets_folder))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "import spacy\n",
                "# !python -m spacy download en_core_web_sm\n",
                "nlp = spacy.load('en_core_web_sm')\n",
                "\n",
                "def run_vader(textual_unit,\n",
                "              sentiments,\n",
                "              lemmatize=False, \n",
                "              parts_of_speech_to_consider=None,\n",
                "              ):\n",
                "    \n",
                "    scores = []\n",
                "    for text in textual_unit:\n",
                "        doc = nlp(str(text))\n",
                "            \n",
                "        input_to_vader = []\n",
                "\n",
                "        for sent in doc.sents:\n",
                "            for token in sent:\n",
                "\n",
                "                to_add = token.text\n",
                "\n",
                "                if lemmatize:\n",
                "                    to_add = token.lemma_\n",
                "\n",
                "                    if to_add == '-PRON-': \n",
                "                        to_add = token.text\n",
                "\n",
                "                if parts_of_speech_to_consider:\n",
                "                    if token.pos_ in parts_of_speech_to_consider:\n",
                "                        input_to_vader.append(to_add) \n",
                "                else:\n",
                "                    input_to_vader.append(to_add)\n",
                "        score = vader_output_to_label(analyzer.polarity_scores(' '.join(input_to_vader)))\n",
                "        scores.append(score)\n",
                "\n",
                "    print(classification_report(sentiments, scores, zero_division=.0))\n",
                "    # return scores"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 75,
            "metadata": {},
            "outputs": [],
            "source": [
                "text_of_tweets = airline_tweets_train.data\n",
                "sent_of_tweets = []\n",
                "for i in airline_tweets_train.target:\n",
                "    sent_of_tweets.append(airline_tweets_train.target_names[i])\n",
                "# sent_of_tweets = airline_tweets_train.target"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 76,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.80      0.51      0.63      1750\n",
                        "     neutral       0.60      0.51      0.55      1515\n",
                        "    positive       0.56      0.88      0.68      1490\n",
                        "\n",
                        "    accuracy                           0.63      4755\n",
                        "   macro avg       0.65      0.63      0.62      4755\n",
                        "weighted avg       0.66      0.63      0.62      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER (as it is) on the set of airline tweets \n",
                "\n",
                "run_vader(text_of_tweets, sent_of_tweets)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 77,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.79      0.52      0.63      1750\n",
                        "     neutral       0.60      0.49      0.54      1515\n",
                        "    positive       0.56      0.88      0.68      1490\n",
                        "\n",
                        "    accuracy                           0.62      4755\n",
                        "   macro avg       0.65      0.63      0.62      4755\n",
                        "weighted avg       0.65      0.62      0.62      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets after having lemmatized the text\n",
                "run_vader(text_of_tweets, sent_of_tweets, lemmatize=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 78,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only adjectives\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"JJ\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 79,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"JJ\"], lemmatize=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 80,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only nouns\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"NN\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 81,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"NN\"], lemmatize=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 82,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only verbs\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"VB\"])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 83,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.00      0.00      0.00      1750\n",
                        "     neutral       0.32      1.00      0.48      1515\n",
                        "    positive       0.00      0.00      0.00      1490\n",
                        "\n",
                        "    accuracy                           0.32      4755\n",
                        "   macro avg       0.11      0.33      0.16      4755\n",
                        "weighted avg       0.10      0.32      0.15      4755\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
                "run_vader(text_of_tweets, sent_of_tweets, parts_of_speech_to_consider=[\"VB\"], lemmatize=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part II: scikit-learn assignments\n",
                "### [4 points] Question 5\n",
                "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
                "\n",
                "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
                "+ Train with different settings:\n",
                "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
                "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
                "* [1 point] a. Generate a classification_report for all experiments\n",
                "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
                "    + which category performs best, is this the case for any setting?\n",
                "    + does the frequency threshold affect the scores? Why or why not according to you?\n",
                "\n",
                "B. While they both perform similar (TF-IDF and BoW) BoW just barely outperforms it. The results of changing ```min_df``` had minimal impact on performance.\n",
                "The result of the TF-IDF is explained by its inner workings. Some tweets may have fewer words to describe the context necessary for it to work well,\n",
                "this leads to it either emphasizing or ignoring critical portions of the text. Whereas BoW simply keeps track of all the \"sentimental\" words' frequencies,\n",
                "which allows it to conclude correlations (in this) quite well."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 87,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.model_selection import train_test_split\n",
                "from nltk.corpus import stopwords\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.feature_extraction.text import TfidfTransformer\n",
                "\n",
                "def tests5(use_tfidf, min_df):\n",
                "    # Train a Multimoda Naive Bayes classifier\n",
                "    airplane_vec = CountVectorizer(min_df=min_df, # If a token appears fewer times than this, across all documents, it will be ignored\n",
                "                                tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
                "                                stop_words=stopwords.words('english')) # stopwords are removed\n",
                "    to_use = airplane_vec.fit_transform(airline_tweets_train.data)\n",
                "\n",
                "    if use_tfidf:\n",
                "        tfidf_transformer = TfidfTransformer()\n",
                "        to_use = tfidf_transformer.fit_transform(to_use)\n",
                "\n",
                "    docs_train, docs_test, y_train, y_test = train_test_split(\n",
                "        to_use, # the tf-idf model\n",
                "        airline_tweets_train.target,\n",
                "        test_size = 0.20 # we use 80% for training and 20% for testing\n",
                "    ) \n",
                "\n",
                "    clf = MultinomialNB().fit(docs_train, y_train)\n",
                "    pred = clf.predict(docs_test)\n",
                "    print(classification_report(y_test, pred, zero_division=.0))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 88,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.87      0.91      0.89       349\n",
                        "           1       0.88      0.77      0.82       308\n",
                        "           2       0.84      0.89      0.87       294\n",
                        "\n",
                        "    accuracy                           0.86       951\n",
                        "   macro avg       0.86      0.86      0.86       951\n",
                        "weighted avg       0.86      0.86      0.86       951\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Bag of words\n",
                "tests5(False, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 91,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.84      0.92      0.88       371\n",
                        "           1       0.87      0.71      0.78       316\n",
                        "           2       0.77      0.84      0.80       264\n",
                        "\n",
                        "    accuracy                           0.83       951\n",
                        "   macro avg       0.83      0.82      0.82       951\n",
                        "weighted avg       0.83      0.83      0.82       951\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "tests5(True, 2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 89,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.80      0.88      0.84       327\n",
                        "           1       0.84      0.73      0.78       332\n",
                        "           2       0.83      0.87      0.85       292\n",
                        "\n",
                        "    accuracy                           0.82       951\n",
                        "   macro avg       0.82      0.82      0.82       951\n",
                        "weighted avg       0.82      0.82      0.82       951\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "tests5(True, 5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 90,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:521: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
                        "  warnings.warn(\n",
                        "c:\\dev\\anaconda3\\envs\\PCA25\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:406: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
                        "  warnings.warn(\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "           0       0.82      0.89      0.85       345\n",
                        "           1       0.79      0.74      0.76       303\n",
                        "           2       0.84      0.82      0.83       303\n",
                        "\n",
                        "    accuracy                           0.82       951\n",
                        "   macro avg       0.82      0.81      0.81       951\n",
                        "weighted avg       0.82      0.82      0.81       951\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "tests5(True, 10)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [4 points] Question 6: Inspecting the best scoring features \n",
                "\n",
                "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
                "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
                "* [3 points] b. Look at the lists and consider the following issues: \n",
                "    + [1 point] Which features did you expect for each separate class and why?\n",
                "    + [1 point] Which features did you not expect and why ? \n",
                "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "              precision    recall  f1-score   support\n",
                        "\n",
                        "    negative       0.84      0.92      0.88       352\n",
                        "     neutral       0.86      0.74      0.80       281\n",
                        "    positive       0.84      0.85      0.85       318\n",
                        "\n",
                        "    accuracy                           0.84       951\n",
                        "   macro avg       0.85      0.84      0.84       951\n",
                        "weighted avg       0.84      0.84      0.84       951\n",
                        "\n",
                        "Important words in negative documents:\n",
                        "  negative 1378.0 united\n",
                        "  negative 372.0 flight\n",
                        "  negative 107.0 service\n",
                        "  negative 103.0 virginamerica\n",
                        "  negative 93.0  get\n",
                        "  negative 91.0  customer\n",
                        "  negative 89.0  bag\n",
                        "  negative 87.0  delayed\n",
                        "  negative 86.0  cancelled\n",
                        "  negative 85.0  time\n",
                        "  negative 83.0  plane\n",
                        "  negative 73.0  hours\n",
                        "  negative 70.0  gate\n",
                        "  negative 65.0  http\n",
                        "  negative 62.0  still\n",
                        "  negative 59.0  would\n",
                        "  negative 58.0  late\n",
                        "  negative 57.0  hour\n",
                        "  negative 56.0  airline\n",
                        "  negative 54.0  one\n",
                        "----------------------------------------\n",
                        "Important words in neutral documents:\n",
                        "   neutral 316.0 jetblue\n",
                        "   neutral 277.0 southwestair\n",
                        "   neutral 266.0 united\n",
                        "   neutral 225.0 flight\n",
                        "   neutral 186.0 americanair\n",
                        "   neutral 180.0 http\n",
                        "   neutral 157.0 usairways\n",
                        "   neutral 84.0  get\n",
                        "   neutral 72.0  virginamerica\n",
                        "   neutral 68.0  flights\n",
                        "   neutral 63.0  please\n",
                        "   neutral 59.0  help\n",
                        "   neutral 53.0  need\n",
                        "   neutral 42.0  tomorrow\n",
                        "   neutral 42.0  dm\n",
                        "   neutral 39.0  would\n",
                        "   neutral 38.0  fleet\n",
                        "   neutral 38.0  fleek\n",
                        "   neutral 37.0  know\n",
                        "   neutral 35.0  thanks\n",
                        "----------------------------------------\n",
                        "Important words in positive documents:\n",
                        "  positive 301.0 southwestair\n",
                        "  positive 294.0 jetblue\n",
                        "  positive 285.0 thanks\n",
                        "  positive 251.0 thank\n",
                        "  positive 240.0 united\n",
                        "  positive 174.0 flight\n",
                        "  positive 171.0 americanair\n",
                        "  positive 126.0 usairways\n",
                        "  positive 125.0 great\n",
                        "  positive 83.0  virginamerica\n",
                        "  positive 83.0  service\n",
                        "  positive 71.0  http\n",
                        "  positive 70.0  love\n",
                        "  positive 70.0  guys\n",
                        "  positive 70.0  best\n",
                        "  positive 66.0  much\n",
                        "  positive 61.0  customer\n",
                        "  positive 50.0  awesome\n",
                        "  positive 49.0  time\n",
                        "  positive 49.0  airline\n",
                        "----------------------------------------\n"
                    ]
                }
            ],
            "source": [
                "import os\n",
                "import nltk\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.tokenize import word_tokenize\n",
                "from sklearn.feature_extraction.text import CountVectorizer\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.naive_bayes import MultinomialNB\n",
                "from sklearn.metrics import classification_report\n",
                "\n",
                "# nltk.download('punkt')\n",
                "# nltk.download('stopwords')\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "tweet_texts = []\n",
                "tweet_labels = []\n",
                "base_dir = \"airlinetweets/airlinetweets\"  \n",
                "\n",
                "for sentiment_label in ['positive', 'neutral', 'negative']:\n",
                "    sentiment_folder = os.path.join(base_dir, sentiment_label)\n",
                "    if not os.path.isdir(sentiment_folder):\n",
                "        print(f\"Folder not found: {sentiment_folder}\")\n",
                "        continue\n",
                "    for filename in os.listdir(sentiment_folder):\n",
                "        if filename.endswith(\".txt\"):\n",
                "            file_path = os.path.join(sentiment_folder, filename)\n",
                "            with open(file_path, 'r', encoding='utf-8') as f:\n",
                "                tweet = f.read().strip().lower()\n",
                "                tokens = word_tokenize(tweet)\n",
                "                filtered_tokens = [w for w in tokens if w.isalpha() and w not in stop_words]\n",
                "                if filtered_tokens:\n",
                "                    tweet_texts.append(\" \".join(filtered_tokens))\n",
                "                    tweet_labels.append(sentiment_label)\n",
                "\n",
                "#print(f\"Loaded {len(tweet_texts)} tweets.\")\n",
                "\n",
                "vectorizer = CountVectorizer(min_df=2)\n",
                "tweet_counts = vectorizer.fit_transform(tweet_texts)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(tweet_counts, tweet_labels, test_size=0.2)\n",
                "\n",
                "clf = MultinomialNB().fit(X_train, y_train)\n",
                "\n",
                "pred = clf.predict(X_test)\n",
                "print(classification_report(y_test, pred, zero_division=0))\n",
                "\n",
                "def important_features_per_class(vectorizer, classifier, n=20):\n",
                "    class_labels = classifier.classes_\n",
                "    feature_names = vectorizer.get_feature_names_out()\n",
                "    for i, label in enumerate(class_labels):\n",
                "        topn = sorted(zip(classifier.feature_count_[i], feature_names), reverse=True)[:n]\n",
                "        print(f\"Important words in {label} documents:\")\n",
                "        for coef, feat in topn:\n",
                "            print(f\"{label:>10} {coef:<5.1f} {feat}\")\n",
                "        print(\"-\" * 40)\n",
                "\n",
                "important_features_per_class(vectorizer, clf, n=20)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "part b\n",
                "\n",
                "1. Our expectations are in line with many of the features in the list of important words for each sentiment class. An example of this is in the negative class. Words such as “terrible”, “n’t” and “fuck” are sensible because they show strong negative emotions or complaints. They are common in tweets that express dissatisfaction. In the positive class, words like “wishes”, “!” and “top” stood out. These words are most often used when someone wants to convey excitement, compliments or celebrations. In the case of neutral tweets, we expected to see words that are more factually or emotionally neutral like “think”, “watch” and “donald”. With the new dataset, we now saw more airline-specific terms appearing. For example, \"delayed\", \"cancelled\" and \"customer\" in negative tweets, and \"thanks\", \"great\" and \"awesome\" in positive tweets. This better reflects the sentiment being analyzed within the airline context.\n",
                "\n",
                "2. We were surprised by some features in the list, as they were less clearly linked to sentiment. For example, we didn’t expect to see the high frequency of punctuation like colons, quotation marks and commas. This is because symbols don’t express any sentiment. We were also surprised by the fact that usernames, links and generic names such as “j” were present. These words are more topic related and don’t particularly have a positive or negative tone. Words like “china” and “language”, which appeared in the negative class were also unexpected. The tone is probably highly influenced by the context they were used in. Similarly, in the new dataset, airline names and customer service phrases were often frequent across multiple classes, which can make interpretation slightly more difficult.\n",
                "\n",
                "3. If we were to improve the model, we would want to remove some of the louder and unnecessary features. For example quotation marks, links, user handles and common names, as they are often related to the topic rather than the sentiment. On the other hand, we would definitely keep words that are emotionally charged. Words such as “terrible” and “fuck”. This is because they carry strong sentimental signals. For expressing strong emotions, words like “n’t” can be useful. Through keeping these kinds of emotionally relevant words, the model will be able to better separate positive, negative and neutral tweets. Additionally, filtering out airline names may help the model rely more on emotional and contextual words rather than brand mentions."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
                "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
                "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
                "* [1 point] c. Discuss whether the model achieved what you expected."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## End of this notebook"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
